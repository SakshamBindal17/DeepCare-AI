# Machine Learning Model - Implementation Guide

## Quick Reference

**Model Type:** Random Forest Classifier  
**Task:** Multi-class classification (Low/Moderate/Critical)  
**Training Data:** 5,000-10,000 FAERS cases  
**Target Accuracy:** â‰¥85%  
**Integration:** Ensemble with rule-based engine

---

## ðŸš€ Quick Start (30-Minute Implementation)

### Step 1: Install Dependencies

```bash
cd backend
pip install scikit-learn pandas numpy joblib
```

### Step 2: Create Directory Structure

```bash
mkdir -p ml models analysis/results
touch ml/__init__.py ml/train_model.py ml/feature_engineering.py
```

### Step 3: Prepare Training Data

```python
# ml/prepare_data.py
import pandas as pd
import requests

def collect_faers_samples(num_samples=1000):
    """Quick data collection from FAERS API"""
    samples = []

    # Top drugs to query
    drugs = ["aspirin", "lisinopril", "metformin", "warfarin", "ibuprofen"]
    symptoms = ["chest pain", "dizziness", "nausea", "rash", "headache"]

    for drug in drugs:
        for symptom in symptoms:
            url = "https://api.fda.gov/drug/event.json"
            params = {
                'search': f'patient.drug.medicinalproduct:"{drug}" AND patient.reaction.reactionmeddrapt:"{symptom}"',
                'limit': 10
            }

            try:
                response = requests.get(url, params=params, timeout=10)
                data = response.json()

                if 'results' in data:
                    for result in data['results'][:5]:
                        samples.append({
                            'drug': drug,
                            'symptom': symptom,
                            'reports': data['meta']['results']['total'],
                            'serious': 1 if result.get('serious', 0) else 0
                        })
            except:
                continue

    df = pd.DataFrame(samples)
    df.to_csv('analysis/results/training_data.csv', index=False)
    return df

# Run this first
if __name__ == '__main__':
    df = collect_faers_samples()
    print(f"Collected {len(df)} samples")
```

### Step 4: Train Model (Minimal Version)

```python
# ml/train_model.py
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import joblib

def train_minimal_model():
    # Load data
    df = pd.read_csv('analysis/results/training_data.csv')

    # Simple feature engineering
    le_drug = LabelEncoder()
    le_symptom = LabelEncoder()

    df['drug_encoded'] = le_drug.fit_transform(df['drug'])
    df['symptom_encoded'] = le_symptom.fit_transform(df['symptom'])

    # Features and labels
    X = df[['drug_encoded', 'symptom_encoded', 'reports']]
    y = df['serious']

    # Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # Train
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # Evaluate
    accuracy = model.score(X_test, y_test)
    print(f"Accuracy: {accuracy:.2%}")

    # Save
    joblib.dump(model, 'models/risk_classifier.pkl')
    joblib.dump(le_drug, 'models/drug_encoder.pkl')
    joblib.dump(le_symptom, 'models/symptom_encoder.pkl')

    return model

if __name__ == '__main__':
    train_minimal_model()
```

### Step 5: Create Prediction Service

```python
# ml_service.py (in backend root)
import joblib
import numpy as np

class MLPredictionService:
    def __init__(self):
        try:
            self.model = joblib.load('models/risk_classifier.pkl')
            self.drug_encoder = joblib.load('models/drug_encoder.pkl')
            self.symptom_encoder = joblib.load('models/symptom_encoder.pkl')
            self.available = True
        except:
            self.available = False
            print("ML model not loaded")

    def predict_risk(self, entities, faers_data):
        if not self.available:
            return None

        # Extract features
        drugs = [e['Text'].lower() for e in entities if e['Category'] == 'MEDICATION']
        symptoms = [e['Text'].lower() for e in entities if e['Category'] == 'MEDICAL_CONDITION']

        if not drugs or not symptoms:
            return None

        # Encode (handle unknown values)
        try:
            drug_encoded = self.drug_encoder.transform([drugs[0]])[0]
            symptom_encoded = self.symptom_encoder.transform([symptoms[0]])[0]
        except:
            return None

        reports = faers_data.get('total_reports', 0)

        # Predict
        X = np.array([[drug_encoded, symptom_encoded, reports]])
        prediction = self.model.predict(X)[0]
        proba = self.model.predict_proba(X)[0]

        return {
            'ml_prediction': 'Critical' if prediction == 1 else 'Low Risk',
            'ml_confidence': float(max(proba)),
            'ml_available': True
        }
```

### Step 6: Integrate into API

```python
# In app.py, add after imports:
from ml_service import MLPredictionService

# Initialize
try:
    ml_service = MLPredictionService()
except:
    ml_service = None

# In analyze_audio endpoint, after risk calculation:
if ml_service and ml_service.available:
    ml_result = ml_service.predict_risk(unique_entities, {'total_reports': total_reports})
    if ml_result:
        response['ml_analysis'] = ml_result
```

---

## ðŸ“Š Full Implementation (Production-Ready)

### Complete Feature Engineering

```python
# ml/feature_engineering.py
import pandas as pd
import numpy as np

class FeatureEngineer:
    def __init__(self):
        self.critical_symptoms = [
            "chest pain", "anaphylaxis", "shortness of breath",
            "difficulty breathing", "stroke", "heart attack"
        ]
        self.moderate_symptoms = [
            "rash", "vomiting", "dizziness", "nausea", "fever"
        ]

    def extract_features(self, entities, faers_data, call_data=None):
        """Extract comprehensive feature set"""

        drugs = [e for e in entities if e['Category'] == 'MEDICATION']
        symptoms = [e for e in entities if e['Category'] == 'MEDICAL_CONDITION']
        anatomy = [e for e in entities if e['Category'] == 'ANATOMY']

        features = {
            # Count features
            'num_drugs': len(drugs),
            'num_symptoms': len(symptoms),
            'num_anatomy': len(anatomy),

            # FAERS features
            'faers_reports': faers_data.get('total_reports', 0),
            'faers_log': np.log1p(faers_data.get('total_reports', 0)),

            # Symptom severity flags
            'has_critical_symptom': any(
                s['Text'].lower() in self.critical_symptoms
                for s in symptoms
            ),
            'has_moderate_symptom': any(
                s['Text'].lower() in self.moderate_symptoms
                for s in symptoms
            ),

            # Confidence scores
            'avg_entity_confidence': np.mean([e.get('Score', 0.8) for e in entities]),
            'min_entity_confidence': np.min([e.get('Score', 0.8) for e in entities]),

            # Frequency features
            'max_symptom_frequency': max([e.get('Frequency', 1) for e in symptoms]) if symptoms else 0,
            'total_entity_mentions': sum([e.get('Frequency', 1) for e in entities]),

            # Polypharmacy
            'polypharmacy_flag': 1 if len(drugs) > 1 else 0,

            # Derived features
            'symptom_drug_ratio': len(symptoms) / max(len(drugs), 1),
            'faers_risk_score': min(faers_data.get('total_reports', 0) / 100, 10),
        }

        # Add call-specific features if available
        if call_data:
            features['call_duration'] = call_data.get('duration', 0)
            features['entity_density'] = len(entities) / max(call_data.get('duration', 1), 1)

        return features
```

### Advanced Model Training

```python
# ml/train_advanced.py
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import joblib

def train_production_model():
    # Load data
    df = pd.read_csv('analysis/results/training_data.csv')

    # Feature engineering
    engineer = FeatureEngineer()
    # ... apply to all rows ...

    # Prepare features
    feature_cols = [col for col in df.columns if col not in ['label', 'case_id']]
    X = df[feature_cols]
    y = df['label']  # 0=Low, 1=Moderate, 2=Critical

    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Hyperparameter tuning
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 15, 20],
        'min_samples_split': [5, 10, 20],
        'class_weight': ['balanced', None]
    }

    rf = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)
    grid_search.fit(X_scaled, y)

    best_model = grid_search.best_estimator_

    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best CV score: {grid_search.best_score_:.2%}")

    # Cross-validation
    cv_scores = cross_val_score(best_model, X_scaled, y, cv=5, scoring='accuracy')
    print(f"CV Accuracy: {cv_scores.mean():.2%} (+/- {cv_scores.std()*2:.2%})")

    # Final evaluation
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.15, stratify=y)

    best_model.fit(X_train, y_train)
    y_pred = best_model.predict(X_test)

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Low', 'Moderate', 'Critical']))

    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    print("\nTop 10 Features:")
    print(feature_importance.head(10))

    # Save everything
    joblib.dump(best_model, 'models/risk_classifier.pkl')
    joblib.dump(scaler, 'models/feature_scaler.pkl')
    joblib.dump(feature_cols, 'models/feature_names.pkl')

    return best_model, feature_importance

if __name__ == '__main__':
    model, importance = train_production_model()
```

---

## ðŸ”„ Ensemble Integration

### Hybrid Prediction Engine

```python
# logic/ensemble_engine.py
from logic.risk_engine import RiskEngine
from ml_service import MLPredictionService

class EnsembleRiskEngine:
    def __init__(self):
        self.rule_engine = RiskEngine()
        self.ml_service = MLPredictionService()

        # Weights (tune based on validation)
        self.rule_weight = 0.6
        self.ml_weight = 0.4

    def calculate_risk(self, entities, faers_data):
        # Rule-based prediction
        rule_result = self.rule_engine.calculate_risk(entities, faers_data)

        # ML prediction
        ml_result = self.ml_service.predict_risk(entities, faers_data)

        if not ml_result or not ml_result.get('ml_available'):
            # Fallback to rule-based only
            return {**rule_result, 'ml_available': False, 'ensemble': False}

        # Convert predictions to numeric scores
        level_to_score = {'Low Risk': 3, 'Moderate': 6, 'Critical': 9}

        rule_score = rule_result['score']
        ml_score = level_to_score.get(ml_result['ml_prediction'], 5)

        # Weighted ensemble
        ensemble_score = (
            self.rule_weight * rule_score +
            self.ml_weight * ml_score
        )

        # Determine final level
        if ensemble_score >= 8:
            final_level = 'Critical'
        elif ensemble_score >= 5:
            final_level = 'Moderate'
        else:
            final_level = 'Low Risk'

        # Calculate confidence
        agreement = (rule_result['level'] == ml_result['ml_prediction'])
        confidence = 'high' if agreement else 'moderate'

        return {
            'score': round(ensemble_score, 1),
            'level': final_level,
            'confidence': confidence,
            'rule_based': {
                'score': rule_score,
                'level': rule_result['level']
            },
            'ml_based': {
                'score': ml_score,
                'level': ml_result['ml_prediction'],
                'confidence': ml_result['ml_confidence']
            },
            'ensemble': True,
            'ml_available': True,
            'action_plan': rule_result['action_plan']  # Use rule-based action plan
        }
```

---

## ðŸŽ¯ Testing & Validation

### Create Test Suite

```python
# ml/test_model.py
import unittest
from ml_service import MLPredictionService

class TestMLModel(unittest.TestCase):
    def setUp(self):
        self.ml_service = MLPredictionService()

    def test_critical_case(self):
        """Test that chest pain + aspirin predicts critical"""
        entities = [
            {'Text': 'Aspirin', 'Category': 'MEDICATION'},
            {'Text': 'chest pain', 'Category': 'MEDICAL_CONDITION'}
        ]
        faers_data = {'total_reports': 500}

        result = self.ml_service.predict_risk(entities, faers_data)

        self.assertIsNotNone(result)
        self.assertEqual(result['ml_prediction'], 'Critical')
        self.assertGreater(result['ml_confidence'], 0.7)

    def test_low_risk_case(self):
        """Test that mild headache predicts low"""
        entities = [
            {'Text': 'ibuprofen', 'Category': 'MEDICATION'},
            {'Text': 'headache', 'Category': 'MEDICAL_CONDITION'}
        ]
        faers_data = {'total_reports': 50}

        result = self.ml_service.predict_risk(entities, faers_data)

        self.assertIsNotNone(result)
        self.assertEqual(result['ml_prediction'], 'Low Risk')

if __name__ == '__main__':
    unittest.main()
```

---

## ðŸ“š Complete File Structure

```
backend/
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prepare_data.py          # Quick data collection
â”‚   â”œâ”€â”€ feature_engineering.py   # Feature extraction
â”‚   â”œâ”€â”€ train_model.py           # Basic training
â”‚   â”œâ”€â”€ train_advanced.py        # Production training
â”‚   â””â”€â”€ test_model.py            # Unit tests
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ risk_classifier.pkl      # Trained model
â”‚   â”œâ”€â”€ feature_scaler.pkl       # StandardScaler
â”‚   â”œâ”€â”€ drug_encoder.pkl         # LabelEncoder for drugs
â”‚   â””â”€â”€ symptom_encoder.pkl      # LabelEncoder for symptoms
â”œâ”€â”€ logic/
â”‚   â”œâ”€â”€ risk_engine.py           # Original rule-based
â”‚   â””â”€â”€ ensemble_engine.py       # Hybrid ensemble
â”œâ”€â”€ ml_service.py                # Prediction service
â””â”€â”€ app.py                       # Flask API (updated)
```

---

## âš¡ Performance Optimization

### Caching Predictions

```python
from functools import lru_cache

class MLPredictionService:
    @lru_cache(maxsize=100)
    def predict_risk_cached(self, drug_tuple, symptom_tuple, faers_count):
        # Convert back to lists
        drugs = list(drug_tuple)
        symptoms = list(symptom_tuple)
        # ... prediction logic ...
```

### Batch Predictions

```python
def predict_batch(self, cases):
    """Predict multiple cases at once"""
    features = [self._extract_features(case) for case in cases]
    X = np.array(features)
    predictions = self.model.predict(X)
    return predictions
```

---

## ðŸŽ“ Presentation Script

**Demo Flow:**

1. **Show Rule-Based Result:**

   > "Our rule-based engine identifies this as Critical with score 9.5"

2. **Show ML Prediction:**

   > "Our machine learning model, trained on 5,000 FAERS cases, also predicts Critical with 94% confidence"

3. **Show Ensemble:**

   > "The ensemble system combines both, giving final score 9.2 with 'high confidence' since both models agree"

4. **Handle Disagreement:**
   > "When models disagree, we flag for human reviewâ€”combining explainability with predictive power"

---

**Status:** Ready for implementation  
**Estimated Time:** 8-12 hours for full production version  
**Quick Version:** 2-3 hours for minimal working model
