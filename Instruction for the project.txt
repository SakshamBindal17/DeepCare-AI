Project DeepCare AI: Detailed Technical Specifications
1. Core Concept & Problem Alignment
DeepCare AI is a specialized clinical safety tool designed to solve Use Case 1 (Adverse Medical Event Prediction). In modern healthcare, crucial signals of patient distress or drug reactions are often buried in hours of recorded phone conversations between patients and nurses.

The Problem: Human error or fatigue can lead to missing "red flags" (Adverse Events) that could lead to hospitalization or death. The Solution: An automated pipeline that transcribes these calls, uses Medical NLP to extract symptoms, and cross-references them with global safety data (FAERS) to provide a real-time risk assessment.

2. The Functional Architecture (The "Ears" to "Brain" Pipeline)
The system must be built as a modular pipeline to ensure high performance and technical transparency:

A. Audio Ingestion (Deepgram Integration): The system must handle asynchronous audio uploads. It utilizes Deepgramâ€™s Speech-to-Text API to convert medical dialogue into text. It must support high-fidelity transcription to distinguish between drug names that sound similar (e.g., Celebrex vs. Celexa).

B. Clinical Entity Extraction (AWS Medical Comprehend): The raw text is processed via Natural Language Processing (NLP). The AI must identify:

Medication: The specific drug the patient is taking.

Symptom/Sign: The patient's reported physical reaction (e.g., "shortness of breath," "nausea").

Anatomy: The part of the body affected.

C. The "Safety Library" (FAERS Data Logic): The system does not guess. It cross-references the extracted Medication + Symptom pair against a localized dataset derived from the FDA Adverse Event Reporting System (FAERS). If a patient mentions "Dizziness" while on "Lisinopril," the system checks the FAERS frequency to determine if this is a known dangerous side effect.

3. The "Traffic Light" Risk Scoring Logic
The system must calculate a Severity Score (1-10) and map it to a visual "Traffic Light" status:

ðŸ”´ RED (Critical Risk): Triggered when life-threatening symptoms are detected (e.g., Anaphylaxis, Chest Pain, Seizures) or when FAERS data shows a high frequency of "Serious Outcomes" for the mentioned drug.

ðŸŸ¡ YELLOW (Moderate Risk): Triggered for non-life-threatening but concerning reactions (e.g., Persistent Vomiting, Rashes, Fainting) that require nurse intervention within 24 hours.

ðŸŸ¢ GREEN (Low/Routine): Triggered when common, expected side effects are mentioned (e.g., dry mouth) or when the patient reports "no symptoms."

4. Unique Modular Features (The "Wow" Factor)
To ensure this project stands out for the judges, the following features must be integrated:

Timestamped Evidence: The UI must link every "Red Flag" directly to the timestamp in the audio. (Example: "Risk detected at 02:14 in the call").

Visual Evidence (FAERS Charts): The dashboard must show a small bar chart comparing the current patient's symptoms to the most common symptoms reported in the FAERS database for that specific drug.

Nurse-Action-Plan: For every Red or Yellow alert, the system generates a suggested "Next Step" for the medical agent (e.g., "Immediately escalate to Physician").

5. Technical Requirements for Implementation
Modularity: Backend logic (API calls) must be strictly separated from the Risk Engine and the Frontend.

Individual Ownership: Code must be structured so that Saksham (Backend/Integration), Mokshit (UI/UX), and Ishan (QA/Research) can contribute to their specific folders without conflict.

Robustness: The system must handle "noisy" medical audio and provide fallback text if the AI confidence score is low.

6. Advanced UI: Synchronized & Intelligent Chat Interface
This feature is the primary way users interact with the transcribed audio. It must be built with "Real-time Synchronization."

A. Chat-Style Layout:

Bubble Format: Use a messaging-style UI (e.g., WhatsApp or iMessage style).

Speaker Diarization: Separate "Patient" (Left, Gray bubbles) and "Nurse/Agent" (Right, Blue bubbles) so the flow of conversation is clear at a glance.

B. Media-Query Synchronization (The "Karaoke" Effect):

Logic: The frontend must listen to the <audio> player's timeupdate event.

Visual Feedback: As the audio plays, the corresponding chat bubble must highlight (e.g., a soft glow or a border change) and auto-scroll to the center of the view.

Interactive Playback: If a user clicks a specific chat bubble, the audio player should jump (seek) to that exact timestamp.

C. Intelligent Word Highlighting (Medical NER Integration):

Bold/Color Logic: Within the chat bubbles, specific medical keywords (Drugs, Symptoms) identified by AWS Medical Comprehend must be Bolded or Colored (e.g., Red for high-risk symptoms).

Tooltip Feature (Optional): Hovering over a bolded word shows its "FAERS Risk Level" (e.g., "Rash: 40% occurrence in FAERS").









Project DeepCare AI: Technology Stack & API Integration
1. Core Tech Stack (The "Glue")
Language: Python 3.12+

Web Framework: Flask (Modular implementation)

Security: python-dotenv for managing API keys and secrets via .env files.

Architecture: Modular design where each API service has its own dedicated class or utility file.

2. Transcription Module (Deepgram API)
Library: deepgram-sdk

Objective: Transcribe pre-recorded medical audio files (.mp3, .wav) with high fidelity.

Copilot Task: Implement the DeepgramClient to use the Nova-2 or Nova-3 model for the best medical-grade transcription accuracy.

Feature Requirement: Ensure smart_format=True is enabled to automatically format medical numbers, dates, and dosages for better readability.

3. Medical NLP Module (AWS Medical Comprehend)
Library: boto3 (AWS SDK for Python)

Service Name: comprehendmedical

Operation: DetectEntitiesV2

Objective: Extract structured clinical data from the transcript.

Extraction Targets: Identify MEDICATION, SYMPTOM, DIAGNOSIS, and DOSAGE.

Copilot Task: Filter the JSON response to specifically find relationships (e.g., which symptom is linked to which medication) and use the Confidence Scores to ensure only high-quality data is used for the risk score.

4. Safety Cross-Reference (openFDA FAERS API)
Source: api.fda.gov

Endpoint: /drug/event.json

Logic: When a drug (e.g., "Lisinopril") and a symptom (e.g., "Dizziness") are found, query the FAERS API using search=patient.drug.medicinalproduct:Lisinopril+AND+patient.reaction.reactionmeddrapt:Dizziness.

Calculation: Compare the frequency of these reactions to establish the "Traffic Light" risk (Red/Yellow/Green).

5. Frontend "Beauty" Requirements
Framework: HTML5 / Tailwind CSS (or React)

Styling Theme: "Modern Clinical" (Color palette: #2D5BFF primary blue, #F5F7FA background).

Components: * Dashboard: A grid layout showing the "Total Calls Analyzed" and "Recent Critical Alerts."

Interactive Transcript View: Highlighting clinical entities in different colors (e.g., Drugs in blue, Symptoms in yellow).

Risk Badge: A pulse animation for RED alerts to draw immediate nurse attention.






Project DeepCare AI: Team Contribution & QA Protocols
1. Individual Contribution Management (GitHub Strategy)
To satisfy the "Multiple Contributors" requirement of the hackathon, the codebase must be strictly organized into individual workspaces. Copilot should help generate specific files for these folders:

Workspace 01 (Saksham - Lead Developer): * Focus: API Integration, Security (.env), and Backend Routing.

Main File: backend/app.py and backend/services/

Workspace 02 (Mokshit - UI/UX Designer):

Focus: Visual Interface, CSS Styling, and Information Architecture.

Main File: frontend/index.html and frontend/styles.css

Workspace 03 (Ishan - QA & Research):

Focus: Data accuracy, FAERS Research, and Testing Scripts.

Main File: tests/test_logic.py and research/faers_dictionary.json

2. Quality Assurance (The "Two-Approach" Requirement)
The project must include two distinct types of testing to be considered technically complete:

Automated Unit Tests: Copilot must generate a script that passes "fake" medical transcripts to the Risk Engine to verify that it correctly identifies "Chest Pain" as a RED alert every single time.

Manual Verification Log: A Markdown file documenting edge-case tests (e.g., "What happens if the audio is silent?" or "What happens if the drug name is misspelled?").

3. Final Presentation & "Beautiful" UI Final Touches
The "Traffic Light" Animation: The UI should not just show a color; it should use a subtle pulse effect for Red alerts to simulate urgency.

Responsive Dashboard: The dashboard must work on both a desktop (for a nurse at a desk) and a tablet (for a nurse on the move).

Interactive Transcript: When a user clicks on a "highlighted" medical term in the text, it should play the audio from that exact second.

How to use this file with Copilot (Prompting Guide)
Once you have saved this entire document as instructions.md in your VS Code, use these prompts to get to work:

Step A (Saksham):

"Read the @instructions.md file. Let's start with Workspace 01. Help me set up the Flask backend in app.py and create the transcription_service.py using the Deepgram details provided."

Step B (Mokshit):

"Look at the Frontend requirements in @instructions.md. Create a 'beautiful' and clinical HTML/Tailwind dashboard for Workspace 02 that includes the Traffic Light and Highlighted Transcript sections."

Step C (Ishan):

"Refer to the QA section in @instructions.md. Generate a Python unit test script in Workspace 03 that validates my Risk Engine logic against common medical symptoms."